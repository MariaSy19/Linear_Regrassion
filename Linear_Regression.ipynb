{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating the dataset\n",
    "label = []\n",
    "ftr =  [[],[],[]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 liner samples\n",
    "for i in range(1000):\n",
    "    X1 = random.random()\n",
    "    ftr[0].append(X1)\n",
    "    X2 = random.random()\n",
    "    ftr[1].append(X2)\n",
    "    X3 = random.random()\n",
    "    ftr[2].append(X3)\n",
    "    Y = 5 * X1 + 3 * X2 + 1.5 * X3 + 6\n",
    "    label.append(Y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.198631</td>\n",
       "      <td>0.038648</td>\n",
       "      <td>0.090877</td>\n",
       "      <td>7.245417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.955702</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.782456</td>\n",
       "      <td>11.961578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.715478</td>\n",
       "      <td>0.324892</td>\n",
       "      <td>0.027087</td>\n",
       "      <td>10.592695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.435368</td>\n",
       "      <td>0.983851</td>\n",
       "      <td>0.991842</td>\n",
       "      <td>12.616155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.380890</td>\n",
       "      <td>0.866892</td>\n",
       "      <td>0.408399</td>\n",
       "      <td>11.117724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.346809</td>\n",
       "      <td>0.440573</td>\n",
       "      <td>0.947150</td>\n",
       "      <td>10.476487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.126149</td>\n",
       "      <td>0.294068</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>7.513957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.396123</td>\n",
       "      <td>0.619179</td>\n",
       "      <td>0.473675</td>\n",
       "      <td>10.548662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>0.814915</td>\n",
       "      <td>0.778199</td>\n",
       "      <td>0.190843</td>\n",
       "      <td>12.695438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>0.517796</td>\n",
       "      <td>0.772686</td>\n",
       "      <td>0.203079</td>\n",
       "      <td>11.211659</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     feature1  feature2  feature3     labels\n",
       "0    0.198631  0.038648  0.090877   7.245417\n",
       "1    0.955702  0.003128  0.782456  11.961578\n",
       "2    0.715478  0.324892  0.027087  10.592695\n",
       "3    0.435368  0.983851  0.991842  12.616155\n",
       "4    0.380890  0.866892  0.408399  11.117724\n",
       "..        ...       ...       ...        ...\n",
       "995  0.346809  0.440573  0.947150  10.476487\n",
       "996  0.126149  0.294068  0.000672   7.513957\n",
       "997  0.396123  0.619179  0.473675  10.548662\n",
       "998  0.814915  0.778199  0.190843  12.695438\n",
       "999  0.517796  0.772686  0.203079  11.211659\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating DataFram to hold the data\n",
    "allData = {'feature1': ftr[0], 'feature2': ftr[1], 'feature3': ftr[2], 'labels': label}\n",
    "dataFram = pd.DataFrame(allData)\n",
    "dataFram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features and labels\n",
    "Xs = dataFram[['feature1', 'feature2', 'feature3']]\n",
    "y = dataFram['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spliting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(Xs, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling using standerScaler\n",
    "# scaling the features to prevent overflow issues in computations\n",
    "# this part avoid feautres bias range of feature (0->1)\n",
    "sclr = StandardScaler()\n",
    "X_train_scled = sclr.fit_transform(X_train)\n",
    "X_test_scled = sclr.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding bias term to X_train for intercept term in linear regression\n",
    "X_bias_train = np.column_stack((np.ones(len(X_train_scled)), X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([5, 3, 1.5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def costFn(weights,x,y):\n",
    "    m = len(y)\n",
    "    predict = x.dot(weights)\n",
    "    sub = np.subtract(predict,y)\n",
    "    squaredErrors = np.square(sub)\n",
    "    cost = 1 / (2 * m) * np.sum(squaredErrors)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent function\n",
    "def gradientDescent(X, Y, weights, LR, iterations):\n",
    "    m = len(Y)\n",
    "    cost_history = []\n",
    "    theta_history = [weights]\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        pred = np.dot(X, weights)\n",
    "        error = pred - Y\n",
    "        cost = costFn(weights,X,Y)\n",
    "        cost_history.append(cost)\n",
    "        # Update weights\n",
    "        weights = weights - (LR * (1 / m) * np.dot(X.T, error))\n",
    "        theta_history.append(weights)\n",
    "\n",
    "    return weights, cost_history, theta_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running gradient descent\n",
    "final_weights, cost_history, theta_history = gradientDescent(X_bias_train, y_train, weights, LR=0.01, iterations=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights: [5.58461032 4.15087742 2.54514193 3.68939931]\n",
      "Loss: 0.22392165752454404\n"
     ]
    }
   ],
   "source": [
    "print(\"Updated weights:\", final_weights)\n",
    "# calculating loss using the final weights and training data\n",
    "loss = costFn(final_weights, X_bias_train, y_train)\n",
    "print(\"Loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
